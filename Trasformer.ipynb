{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cDxgiLtk_Lb"
      },
      "source": [
        "\n",
        "### Attention is all you need\n",
        "### using multi head attention with the feed forward layer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Hyper parameters\n",
        "\n",
        "block_size = 128\n",
        "batch_size = 200\n",
        "max_iters   = 5000\n",
        "eval_interval = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_emb = 256*4\n",
        "learning_rate = 0.0001\n",
        "dropout = 0.2\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "#+-------------------------+\n",
        "#+-------------------------+"
      ],
      "metadata": {
        "id": "n67id_0lIuHa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "data_loc = '/1.Abstract.txt'\n",
        "with open(data_loc, 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "### Encoder (tokenizer) and Decoder\n",
        "stoi = {j:i for i,j in enumerate(chars)}\n",
        "itos = {i:j for i,j in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] ## takes the string outputs the list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "## split the data in train and val\n",
        "n = int(len(data)*0.9)\n",
        "train_data = data[:n]\n",
        "val_data   = data[:n]\n",
        "\n",
        "#------------------------\n"
      ],
      "metadata": {
        "id": "-PpPArpHIw29"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### split data in batches\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix   = torch.randint(len(data) - block_size, (batch_size,)) ## returns random indices till len(data) - block_size -1\n",
        "    x    = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y    = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x,y  = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "## esimate loss (iterating over multiple batches)\n",
        "@torch.no_grad() ## context manager --> not to maintain the map\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X,Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key   = nn.Linear(n_emb, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_emb, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_emb, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) ## according to convention as it is not a parameter\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)    # B,T,C\n",
        "        q = self.query(x)  # B,T,C\n",
        "        ## compute attention scores (affinities)\n",
        "        wei = q @ k.transpose(-2,-1) * C ** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # (B,T,T) ## make this a decoder block\n",
        "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T,C)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "## multi head attention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"Multiple Heads of self attention in parallel\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_emb, n_emb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out =  torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "## feed forward layer -- let the model figure out connections identified by the attention mechanism\n",
        "class FeedForward(nn.Module):\n",
        "    \"a simple linear layer followed by a non linearity\"\n",
        "\n",
        "    def __init__(self, n_emb):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(n_emb,4* n_emb),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_emb, n_emb),\n",
        "        nn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"Transformer Block: Communication followed by computation\"\n",
        "\n",
        "    def __init__(self, n_emb, n_head):\n",
        "        # n_emb - number of embedding dimentions, n_head - num of self attention heads\n",
        "        super().__init__()\n",
        "        head_size = n_emb//n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwf = FeedForward(n_emb)\n",
        "        self.ln1 = nn.LayerNorm(n_emb)\n",
        "        self.ln2 = nn.LayerNorm(n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwf(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "## simple Bigram model\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from the lookup (emb) table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_emb)\n",
        "        self.blocks = nn.Sequential(*[Block(n_emb, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_emb)\n",
        "        self.lm_head1 = nn.Linear(n_emb, n_emb*4)\n",
        "        self.lm_head2 = nn.Linear(n_emb*4, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensors\n",
        "        tok_emb = self.token_embedding_table(idx) # logits --> log(counts) ## (B,T,C) this c  == nemb\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T,C) c == nemb\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x) ## apply one head self attention\n",
        "        x = self.ln_f(x)\n",
        "        x  = self.lm_head1(x)\n",
        "        logits  = self.lm_head2(x)  ## (B,T,C) c == vocab size\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B,T,C  = logits.shape\n",
        "            logits = logits.view(B*T,C) ## Crossentropy takes in B,C\n",
        "            targets = targets.view(B*T) ## Crossentropy takes in B,C\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            ## crop idx to the last block size token\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step --> this will give prediction for each time step\n",
        "            logits = logits[:, -1, :]  # (B, C)\n",
        "            probs = F.softmax(logits, dim = -1) # (B,C)\n",
        "            ## sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T +1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device) ## weights will be moved to GPU\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "urNepU93I540"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create a pytorch optimizer\n",
        "learning_rate = 0.00003\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n",
        "\n",
        "for step in range(max_iters):\n",
        "\n",
        "    ## every once in a while estimate the loss on train and val sets\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # get a mini batch\n",
        "    xb, yb = get_batch('train')\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "### Generate from the model\n",
        "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "print(decode(m.generate(context, max_new_tokens = 1000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccd0F0pZI98i",
        "outputId": "7e12aa6e-8099-48fc-af2c-f1b5ebd75dd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6914, val loss 4.6915\n",
            "step 100: train loss 2.4588, val loss 2.4599\n",
            "step 200: train loss 2.3704, val loss 2.3708\n",
            "step 300: train loss 2.2736, val loss 2.2723\n",
            "step 400: train loss 2.1444, val loss 2.1442\n",
            "step 500: train loss 1.9940, val loss 1.9944\n",
            "step 600: train loss 1.8694, val loss 1.8680\n",
            "step 700: train loss 1.7767, val loss 1.7744\n",
            "step 800: train loss 1.7074, val loss 1.7075\n",
            "step 900: train loss 1.6516, val loss 1.6536\n",
            "step 1000: train loss 1.6090, val loss 1.6090\n",
            "step 1100: train loss 1.5683, val loss 1.5686\n",
            "step 1200: train loss 1.5354, val loss 1.5362\n",
            "step 1300: train loss 1.5063, val loss 1.5062\n",
            "step 1400: train loss 1.4812, val loss 1.4817\n",
            "step 1500: train loss 1.4581, val loss 1.4584\n",
            "step 1600: train loss 1.4390, val loss 1.4366\n",
            "step 1700: train loss 1.4159, val loss 1.4162\n",
            "step 1800: train loss 1.4023, val loss 1.4018\n",
            "step 1900: train loss 1.3853, val loss 1.3868\n",
            "step 2000: train loss 1.3692, val loss 1.3680\n",
            "step 2100: train loss 1.3554, val loss 1.3551\n",
            "step 2200: train loss 1.3421, val loss 1.3409\n",
            "step 2300: train loss 1.3268, val loss 1.3292\n",
            "step 2400: train loss 1.3167, val loss 1.3166\n",
            "step 2500: train loss 1.3056, val loss 1.3054\n",
            "step 2600: train loss 1.2984, val loss 1.2960\n",
            "step 2700: train loss 1.2890, val loss 1.2892\n",
            "step 2800: train loss 1.2799, val loss 1.2784\n",
            "step 2900: train loss 1.2676, val loss 1.2684\n",
            "step 3000: train loss 1.2611, val loss 1.2625\n",
            "step 3100: train loss 1.2559, val loss 1.2534\n",
            "step 3200: train loss 1.2467, val loss 1.2473\n",
            "step 3300: train loss 1.2385, val loss 1.2387\n",
            "step 3400: train loss 1.2301, val loss 1.2301\n",
            "step 3500: train loss 1.2251, val loss 1.2248\n",
            "step 3600: train loss 1.2168, val loss 1.2160\n",
            "step 3700: train loss 1.2096, val loss 1.2093\n",
            "step 3800: train loss 1.2036, val loss 1.2035\n",
            "step 3900: train loss 1.2000, val loss 1.1995\n",
            "step 4000: train loss 1.1938, val loss 1.1916\n",
            "step 4100: train loss 1.1899, val loss 1.1896\n",
            "step 4200: train loss 1.1805, val loss 1.1822\n",
            "step 4300: train loss 1.1761, val loss 1.1760\n",
            "step 4400: train loss 1.1724, val loss 1.1723\n",
            "step 4500: train loss 1.1643, val loss 1.1652\n",
            "step 4600: train loss 1.1630, val loss 1.1623\n",
            "step 4700: train loss 1.1566, val loss 1.1559\n",
            "step 4800: train loss 1.1556, val loss 1.1541\n",
            "step 4900: train loss 1.1478, val loss 1.1460\n",
            "1.211965799331665\n",
            "\tonto Hadmant:\n",
            "\n",
            "One could readers could less and just fund their own where the gives for the does fears.\n",
            "\n",
            "Demost interesting a startup founder made be funancial rely becomes founder, and even on thinking one's contined that's of the largely bruind. And collectualies, have to make the out of the threm.\n",
            "\n",
            "The fact people to ime things that's still forms into discov, I notigity, when I'd never had to seen market growth.\n",
            "\n",
            "The nerded to hack and \"well\" in a long to learn startup,\" investor but was bases worse the other has bestory. \"So areso but a strong as taking used where it's raising shope that soundars by referrming you end things for magins I was be accovered, the founder that if great would in provity from it. [7]\n",
            "In a lattrouble of a left for them completely by give, decident to a startup, to glasse if it was the world probably attacy. I'm still on for have here degoence who beginness someone uni how, succeeded how Isn't eyel have been set ofense. You you're already doing kids always,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(m.state_dict(), '/model_state_dict.pth')"
      ],
      "metadata": {
        "id": "jU2V5s67N-Cz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = torch.load('/model_state_dict.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aujh_CtlsOqY",
        "outputId": "2066e348-bc08-4901-8bc8-ceae756884b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-f33d6b0693ed>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model1 = torch.load('/model_state_dict.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/model_state_dict.pth'\n",
        "torch.save(m.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "iRAVGaNOtzxp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = BigramLanguageModel(vocab_size)\n",
        "m1 = model1.to(device) ## weights will be moved to GPU\n",
        "loaded_state_dict = torch.load('/model_state_dict.pth', map_location=device)  # Or 'cpu' for CPU\n",
        "m1.load_state_dict(loaded_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lZdsy9qsufS",
        "outputId": "59615c8a-8adc-4b43-beaf-aa00ef2c62c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-8627e6f91a60>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  loaded_state_dict = torch.load('/model_state_dict.pth', map_location=device)  # Or 'cpu' for CPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
        "print(decode(m1.generate(context, max_new_tokens = 1000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhS2G2NuDm1",
        "outputId": "ea2c9e98-b899-4ab9-c837-262e50878e33"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\n",
            "How unto large. Grally better Boshelf Carning France.\n",
            "\n",
            "As I don't merely the most of people to get several students of the kist what Y Combinator who give you at a YC near? That's when startup, the power must people to work in finding growth half in \"Applayers in a companyon by temptable to the main returnes. It knows when you'd usee to that getting to students open to do to get the pretty of how how big doefint bore where.\n",
            "In Crobabily 29 (1) that practick are says, someone that is and level the bubby unintreditionalishmen. The deadly my bothfor having grades. But area or friced I almost distractions and with variation flie so grew rate the half of rame goals.\n",
            "\n",
            "[1] Jessica Live, or not real VCs folloo Wozniars\n",
            "\n",
            "Arpuble. By their leves in Lisp Kone 3warses mone, it's forgoing here. If you don't know world be acaptions, that one crowing that. Once you're topics floop mind themselves — for notice, in having example. And individiagring partners. This everyone else domain order.\n",
            "\n",
            "I notick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9oBBICiZuGMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
